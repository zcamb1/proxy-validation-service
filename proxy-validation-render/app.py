from flask import Flask, jsonify, request
import requests
import threading
import time
import json
import os
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
import sys
import traceback
from collections import deque


# Connection pooling for better efficiency on free plan
import requests
session = requests.Session()
session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})

def get_with_session(url, **kwargs):
    try:
        return session.get(url, **kwargs)
    except Exception as e:
        # Fallback to regular requests
        return requests.get(url, **kwargs)

app = Flask(__name__)

# Global log buffer ƒë·ªÉ store logs cho real-time display
log_buffer = deque(maxlen=500)  # Keep last 500 log entries
startup_status = {
    "initialized": False,
    "background_thread_started": False,
    "first_fetch_completed": False,
    "error_count": 0,
    "last_activity": None
}

# Cache proxy s·ªëng
proxy_cache = {
    "http": [],
    "last_update": None,
    "total_checked": 0,
    "alive_count": 0,
    "sources_processed": 0
}

# Thread safety lock
cache_lock = threading.Lock()

# Ngu·ªìn proxy ƒë∆∞·ª£c ph√¢n lo·∫°i v·ªõi protocol r√µ r√†ng - t·ªëi ∆∞u cho Render free plan (ULTRA OPTIMIZED)
PROXY_SOURCE_LINKS = {
    # Categorized sources - m·ªói ngu·ªìn c√≥ protocol c·ª• th·ªÉ
    "categorized": {
        "Server Alpha": {
            "http": "https://cdn.jsdelivr.net/gh/databay-labs/free-proxy-list/http.txt",
            "https": "https://cdn.jsdelivr.net/gh/databay-labs/free-proxy-list/https.txt", 
            "socks5": "https://cdn.jsdelivr.net/gh/databay-labs/free-proxy-list/socks5.txt",
        },
         "Network Beta sock4": {
            "url": "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks4.txt",
            "protocol": "socks4"
        },
         "Network Beta sock5": {
            "url": "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks5.txt",
            "protocol": "socks5"
        },
        "Gateway Pro": {
            "url": "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt", 
            "protocol": "http"
        },
        "Server Delta": {
            "url": "https://raw.githubusercontent.com/hookzof/socks5_list/master/proxy.txt",
            "protocol": "socks5"
        },
        "Server Foxtrot": {
            "url": "https://www.proxy-list.download/api/v1/get?type=http",
            "protocol": "http"
        },
        "Server Golf": {
            "url": "https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt",
            "protocol": "http"
        },

        "Server India": {
            "url": "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt",
            "protocol": "http"
        },
    },
    # Mixed sources - test v·ªõi t·∫•t c·∫£ protocols (http, https, socks4, socks5)
    "mixed": {
        "hendrikbgr": {
            "url": "https://raw.githubusercontent.com/hendrikbgr/Free-Proxy-Repo/master/proxy_list.txt",
            "protocols": ["http", "https", "socks4", "socks5"]
        },
        "MrMarble": {
            "url": "https://raw.githubusercontent.com/MrMarble/proxy-list/main/all.txt",
            "protocols": ["http", "https", "socks4", "socks5"]
        },
        "sunny9577": {
            "url": "https://raw.githubusercontent.com/sunny9577/proxy-scraper/master/proxies.txt",
            "protocols": ["http", "https", "socks4", "socks5"]
        },
        "jetkai": {
            "url": "https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies.txt",
            "protocols": ["http", "https", "socks4", "socks5"]
        },
        "clarketm": {
            "url": "https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt",
            "protocols": ["http", "https"]
        },
        "rdavydov": {
            "url": "https://raw.githubusercontent.com/rdavydov/proxy-list/main/proxies/http.txt",
            "protocols": ["http", "https"]
        },
        "officialputuid": {
            "url": "https://raw.githubusercontent.com/officialputuid/KangProxy/KangProxy/http/http.txt",
            "protocols": ["http", "https"]
        }
    }
}

def log_to_render(message, level="INFO"):
    """Log v·ªõi format r√µ r√†ng cho Render logs v√† web display"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    log_msg = f"[{level}] {timestamp} | {message}"
    
    # Console log
    print(log_msg)
    sys.stdout.flush()
    
    # Web log buffer
    log_buffer.append({
        "timestamp": timestamp,
        "level": level,
        "message": message,
        "full_log": log_msg
    })
    
    # Update activity
    with cache_lock:

        startup_status["last_activity"] = datetime.now().isoformat()

def initialize_service():
    """Initialize service - ƒë∆∞·ª£c g·ªçi khi Flask app start"""
    if startup_status["initialized"]:
        return
        
    try:
        log_to_render("üöÄ KH·ªûI ƒê·ªòNG PROXY VALIDATION SERVICE")
        log_to_render("üîß T·ªëi ∆∞u cho Render free plan (512MB RAM) - ULTRA OPTIMIZED")
        log_to_render("üìã C·∫•u h√¨nh: Timeout=8s, Workers=20, Chunks=200, Max=2000")
        
        # Start background thread
        log_to_render("üîÑ ƒêANG KH·ªûI ƒê·ªòNG BACKGROUND THREAD...")
        try:
            refresh_thread = threading.Thread(target=background_proxy_refresh, daemon=True)
            refresh_thread.start()
            log_to_render("‚úÖ Background thread started!")
            
            # Verify thread is running
            if refresh_thread.is_alive():
                log_to_render("‚úÖ Background thread confirmed ALIVE!")
                with cache_lock:
                    startup_status["background_thread_started"] = True
            else:
                log_to_render("‚ùå Background thread not alive!")
                startup_status["error_count"] += 1
                
        except Exception as e:
            log_to_render(f"‚ùå L·ªñI CRITICAL kh·ªüi ƒë·ªông background thread: {str(e)}")
            startup_status["error_count"] += 1
        
        # Set empty cache initially
        log_to_render("üíæ Setting initial empty cache...")
        with cache_lock:
            proxy_cache["http"] = []
            proxy_cache["last_update"] = datetime.now().isoformat()
            proxy_cache["total_checked"] = 0
            proxy_cache["alive_count"] = 0
            proxy_cache["sources_processed"] = 0
        
        with cache_lock:
            startup_status["initialized"] = True
        log_to_render("‚úÖ SERVICE INITIALIZATION COMPLETED!")
        
    except Exception as e:
        log_to_render(f"‚ùå L·ªñI CRITICAL INITIALIZATION: {str(e)}")
        startup_status["error_count"] += 1


def is_quality_proxy(proxy_string):
    """Basic quality filter for better output"""
    try:
        # Skip obviously bad proxies
        if not proxy_string or len(proxy_string) < 7:
            return False
        
        # Basic format validation
        if ':' not in proxy_string:
            return False
            
        parts = proxy_string.split(':')
        if len(parts) != 2:
            return False
            
        host, port = parts
        
        # Skip localhost, private IPs, invalid ports
        if host.startswith(('127.', '10.', '192.168.', '172.')):
            return False
            
        if not port.isdigit() or int(port) < 1 or int(port) > 65535:
            return False
            
        return True
    except:
        return False

def check_single_proxy(proxy_string, timeout=8, protocols=['http']):
    """Ki·ªÉm tra 1 proxy v·ªõi c√°c protocols kh√°c nhau - t·ªëi ∆∞u cho Render"""
    try:
        if ':' not in proxy_string:
            return None
            
        # Parse proxy format: host:port ho·∫∑c username:password@host:port
        if '@' in proxy_string:
            auth_part, host_port = proxy_string.split('@')
            if ':' in auth_part:
                username, password = auth_part.split(':', 1)
            else:
                username, password = auth_part, ""
        else:
            username, password = None, None
            host_port = proxy_string

        if ':' not in host_port:
            return None
            
        host, port = host_port.strip().split(':', 1)
        
        # Test URLs 
        test_urls = [
            'http://httpbin.org/ip',
            'http://ip-api.com/json',
            'https://api.ipify.org',
        ]
        
        # Test v·ªõi t·ª´ng protocol
        for protocol in protocols:
            try:
                # Setup proxy theo protocol
                if username and password:
                    if protocol in ['socks4', 'socks5']:
                        proxy_url = f"{protocol}://{username}:{password}@{host}:{port}"
                    else:
                        proxy_url = f"http://{username}:{password}@{host}:{port}"
                else:
                    if protocol in ['socks4', 'socks5']:
                        proxy_url = f"{protocol}://{host}:{port}"
                    else:
                        proxy_url = f"http://{host}:{port}"
                
                proxies = {
                    'http': proxy_url,
                    'https': proxy_url
                }
                
                start_time = time.time()
                
                # Test proxy v·ªõi multiple URLs
                for test_url in test_urls:
                    try:
                        response = requests.get(
                            test_url,
                            proxies=proxies,
                            timeout=timeout,
                            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                        )
                        
                        if response.status_code == 200:
                            speed = round(time.time() - start_time, 2)
                            
                            # Get proxy IP
                            try:
                                ip_data = response.json()
                                proxy_ip = ip_data.get('origin', ip_data.get('query', ip_data.get('ip', 'unknown')))
                                # Clean IP (remove port if present)
                                if ',' in proxy_ip:
                                    proxy_ip = proxy_ip.split(',')[0]
                            except Exception:
                                proxy_ip = 'unknown'
                            
                            return {
                                'host': host,
                                'port': int(port),
                                'type': protocol,
                                'speed': speed,
                                'status': 'alive',
                                'ip': proxy_ip,
                                'checked_at': datetime.now().isoformat(),
                                'proxy_string': f"{host}:{port}",
                                'full_proxy': proxy_string,
                                'has_auth': bool(username and password)
                            }
                    except Exception:
                        # REMOVED: B·ªè error logs ƒë·ªÉ gi·∫£m noise
                        continue
                        
            except Exception:
                # REMOVED: B·ªè error logs ƒë·ªÉ gi·∫£m noise
                continue
                
    except Exception:
        # REMOVED: B·ªè error logs ƒë·ªÉ gi·∫£m noise
        pass
    
    return None

def fetch_proxies_from_sources():
    """L·∫•y proxy t·ª´ t·∫•t c·∫£ ngu·ªìn v·ªõi logic th√¥ng minh - t·ªëi ∆∞u cho Render"""
    categorized_proxies = []
    mixed_proxies = []
    sources_processed = 0
    
    log_to_render("üîç B·∫ÆT ƒê·∫¶U FETCH PROXY T·ª™ C√ÅC NGU·ªíN...")
    log_to_render(f"üìã T·ªïng {len(PROXY_SOURCE_LINKS['categorized'])} categorized + {len(PROXY_SOURCE_LINKS['mixed'])} mixed sources")
    
    # X·ª≠ l√Ω Server Alpha tr∆∞·ªõc (∆∞u ti√™n t·ªëi ƒëa ƒë·ªÉ l·∫•y 1000+ proxy)
    log_to_render("üì• X·ª≠ l√Ω SERVER ALPHA tr∆∞·ªõc (∆∞u ti√™n t·ªëi ƒëa)...")
    server_alpha_proxies = []
    
    if "Server Alpha" in PROXY_SOURCE_LINKS["categorized"]:
        try:
            source_config = PROXY_SOURCE_LINKS["categorized"]["Server Alpha"]
            protocols_to_fetch = [(protocol, url) for protocol, url in source_config.items()]
            
            for source_protocol, source_url in protocols_to_fetch:
                # REDUCED: Ch·ªâ log protocol quan tr·ªçng
                
                response = get_with_session(source_url, timeout=45)
                
                if response.status_code == 200:
                    content = response.text
                    lines = content.strip().split('\n')
                    source_proxies = []
                    
                    for line in lines:
                        line = line.strip()
                        if not line or line.startswith('#'):
                            continue
                            
                        if ':' in line and is_quality_proxy(line.strip()):
                            try:
                                if '@' in line:
                                    auth_part, host_port = line.split('@')
                                    host, port = host_port.split(':')
                                else:
                                    host, port = line.split(':')
                                
                                if len(host.split('.')) == 4 and port.isdigit():
                                    source_proxies.append(('categorized', line, source_protocol))
                                    
                            except Exception:
                                continue
                    
                    server_alpha_proxies.extend(source_proxies)
                    log_to_render(f"‚úÖ Server Alpha - {source_protocol}: {len(source_proxies)} proxy")
                else:
                    log_to_render(f"‚ùå Server Alpha - {source_protocol}: HTTP {response.status_code}")
            
            log_to_render(f"üéØ Server Alpha TOTAL: {len(server_alpha_proxies)} proxy from {len(protocols_to_fetch)} protocols")
            sources_processed += 1
            
        except Exception as e:
            log_to_render(f"‚ùå Server Alpha: {str(e)}")
    
    # X·ª≠ l√Ω c√°c categorized sources kh√°c
    log_to_render("üì• X·ª≠ l√Ω CATEGORIZED sources kh√°c...")
    for source_name, source_config in PROXY_SOURCE_LINKS["categorized"].items():
        if source_name == "Server Alpha":
            continue  # ƒê√£ x·ª≠ l√Ω r·ªìi
        try:
            # Check if source has multiple protocols (Server Alpha style) or single protocol
            if "url" in source_config and "protocol" in source_config:
                # Single protocol format
                source_url = source_config["url"]
                source_protocol = source_config["protocol"]
                protocols_to_fetch = [(source_protocol, source_url)]
            else:
                # Multiple protocols format (Server Alpha style)
                protocols_to_fetch = [(protocol, url) for protocol, url in source_config.items()]
            
            source_total_proxies = []
            
            for source_protocol, source_url in protocols_to_fetch:
                # REMOVED: B·ªè log individual protocol
                
                response = get_with_session(source_url, timeout=45)
                
                if response.status_code == 200:
                    content = response.text
                    lines = content.strip().split('\n')
                    source_proxies = []
                    
                    for line in lines:
                        line = line.strip()
                        if not line or line.startswith('#'):
                            continue
                            
                        # Validate proxy format
                        if ':' in line and is_quality_proxy(line.strip()):
                            try:
                                # Check if it's valid proxy format
                                if '@' in line:
                                    auth_part, host_port = line.split('@')
                                    host, port = host_port.split(':')
                                else:
                                    host, port = line.split(':')
                                
                                # Basic validation
                                if len(host.split('.')) == 4 and port.isdigit():
                                    source_proxies.append(('categorized', line, source_protocol))
                                    
                            except Exception:
                                continue
                    
                    source_total_proxies.extend(source_proxies)
                    # REDUCED: Ch·ªâ log n·∫øu c√≥ proxy
                    if len(source_proxies) > 0:
                        log_to_render(f"‚úÖ {source_name} - {source_protocol}: {len(source_proxies)} proxy")
                else:
                    log_to_render(f"‚ùå {source_name} - {source_protocol}: HTTP {response.status_code}")
            
            categorized_proxies.extend(source_total_proxies)
            sources_processed += 1
            log_to_render(f"üéØ {source_name} TOTAL: {len(source_total_proxies)} proxy")
        
        except Exception as e:
            log_to_render(f"‚ùå {source_name}: {str(e)}")
            continue
    
    # X·ª≠ l√Ω mixed sources sau
    log_to_render("üì• X·ª≠ l√Ω MIXED sources...")
    for source_name, source_config in PROXY_SOURCE_LINKS["mixed"].items():
        try:
            source_url = source_config["url"]
            source_protocols = source_config["protocols"]
            # REMOVED: B·ªè log chi ti·∫øt protocols
            
            response = get_with_session(source_url, timeout=45)
            
            if response.status_code == 200:
                content = response.text
                lines = content.strip().split('\n')
                source_proxies = []
                
                for line in lines:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                        
                    # Validate proxy format
                    if ':' in line and is_quality_proxy(line.strip()):
                        try:
                            # Check if it's valid proxy format
                            if '@' in line:
                                auth_part, host_port = line.split('@')
                                host, port = host_port.split(':')
                            else:
                                host, port = line.split(':')
                            
                            # Basic validation  
                            if len(host.split('.')) == 4 and port.isdigit():
                                source_proxies.append(('mixed', line, source_protocols))
                                
                        except Exception:
                            continue
                
                mixed_proxies.extend(source_proxies)
                sources_processed += 1
                log_to_render(f"‚úÖ {source_name}: {len(source_proxies)} proxy")
            else:
                log_to_render(f"‚ùå {source_name}: HTTP {response.status_code}")
        
        except Exception as e:
            log_to_render(f"‚ùå {source_name}: {str(e)}")
            continue
    
    # Combine t·∫•t c·∫£ proxy (Server Alpha + categorized kh√°c + mixed) - KH√îNG GI·ªöI H·∫†N
    all_proxies = server_alpha_proxies + categorized_proxies + mixed_proxies
    random.shuffle(all_proxies)
    
    log_to_render(f"üéØ HO√ÄN TH√ÄNH FETCH: {len(all_proxies)} total proxy (KH√îNG GI·ªöI H·∫†N)")
    log_to_render(f"üìä ƒê√£ x·ª≠ l√Ω {sources_processed} ngu·ªìn th√†nh c√¥ng")
    log_to_render(f"üìã Server Alpha: {len(server_alpha_proxies)}, Categorized kh√°c: {len(categorized_proxies)}, Mixed: {len(mixed_proxies)}")
    
    return all_proxies, sources_processed

def validate_proxy_batch_smart(proxy_list, max_workers=15):
    """Validate proxies KH√îNG chunking - ch·ªâ validate to√†n b·ªô list ƒë∆∞·ª£c pass v√†o"""
    if not proxy_list:
        log_to_render("‚ö†Ô∏è Kh√¥ng c√≥ proxy ƒë·ªÉ validate")
        return []
    
    # S·ª≠ d·ª•ng global cache ƒë·ªÉ update real-time
    global proxy_cache
    alive_proxies = []
    total_proxies = len(proxy_list)
    
    log_to_render(f"‚ö° B·∫ÆT ƒê·∫¶U VALIDATE {total_proxies} PROXY")
    log_to_render(f"üîß C·∫•u h√¨nh: {max_workers} workers (KH√îNG sub-chunking)")
    
    # KH√îNG reset cache c≈©, ch·ªâ track validation hi·ªán t·∫°i
    current_validation_checked = 0
    current_validation_alive = 0
    
    # Gi·ªØ l·∫°i proxy c≈© v√† t√≠ch l≈©y th√™m proxy m·ªõi
    existing_proxies = proxy_cache.get("http", [])
    log_to_render(f"üîÑ B·∫Øt ƒë·∫ßu validation m·ªõi - Gi·ªØ l·∫°i {len(existing_proxies)} proxy c≈©")
    
    # Count proxy types
    categorized_count = sum(1 for item in proxy_list if isinstance(item, tuple) and item[0] == 'categorized')
    mixed_count = sum(1 for item in proxy_list if isinstance(item, tuple) and item[0] == 'mixed')
    
    log_to_render(f"üîß Proxy types: {categorized_count} categorized(specific) + {mixed_count} mixed(all)")
    
    checked_count = 0
    
    # Validate T·∫§T C·∫¢ proxy c√πng l√∫c v·ªõi ThreadPoolExecutor - KH√îNG sub-chunking
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit t·∫•t c·∫£ proxy
        future_to_proxy = {}
        for proxy_data in proxy_list:
            # Unpack proxy data v·ªõi structure m·ªõi
            if isinstance(proxy_data, tuple) and len(proxy_data) == 3:
                proxy_type, proxy_string, protocols_info = proxy_data
            else:
                # Fallback for old format
                proxy_type, proxy_string, protocols_info = 'categorized', proxy_data, 'http'
            
            # X√°c ƒë·ªãnh protocols ƒë·ªÉ test d·ª±a tr√™n source type
            if proxy_type == 'mixed':
                protocols = protocols_info  # Mixed sources s·ª≠ d·ª•ng protocols t·ª´ config
            else:
                protocols = [protocols_info]  # Categorized sources s·ª≠ d·ª•ng protocol c·ª• th·ªÉ
            
            future = executor.submit(check_single_proxy, proxy_string, 8, protocols)
            future_to_proxy[future] = (proxy_type, proxy_string, protocols_info)
        
        # Collect results v·ªõi progress tracking
        for future in as_completed(future_to_proxy):
            checked_count += 1
            current_validation_checked += 1
            proxy_type, proxy_string, protocols_info = future_to_proxy[future]
            
            try:
                result = future.result()
                if result:
                    alive_proxies.append(result)
                    
                    # T√≠ch l≈©y proxy m·ªõi v·ªõi proxy c≈© (tr√°nh duplicate) - l·∫•y real-time
                    with cache_lock:
                        current_proxies = proxy_cache.get("http", []).copy()
                    
                    # Th√™m proxy m·ªõi n·∫øu ch∆∞a c√≥
                    proxy_key = f"{result['host']}:{result['port']}"
                    existing_keys = [f"{p['host']}:{p['port']}" for p in current_proxies]
                    
                    if proxy_key not in existing_keys:
                        current_proxies.append(result)
                    
                    # Update cache v·ªõi danh s√°ch t√≠ch l≈©y
                    with cache_lock:
                        proxy_cache["http"] = current_proxies.copy()
                        proxy_cache["alive_count"] = len(current_proxies)
                        proxy_cache["total_checked"] = proxy_cache.get("total_checked", 0) + 1
                        proxy_cache["last_update"] = datetime.now().isoformat()
                    
                    current_validation_alive += 1
                    
                else:
                    # Update total checked even for failed (t√≠ch l≈©y)
                    with cache_lock:
                        proxy_cache["total_checked"] = proxy_cache.get("total_checked", 0) + 1
                    
                    # REDUCED: Ch·ªâ log progress √≠t h∆°n
                    if checked_count % 100 == 0:  # Log m·ªói 100 proxy
                        progress_pct = round(checked_count/total_proxies*100, 1)
                        log_to_render(f"‚è≥ Progress: {checked_count}/{total_proxies} checked ({progress_pct}%), {len(alive_proxies)} alive")
                        
            except Exception as e:
                # Update total checked even for exceptions (t√≠ch l≈©y)
                with cache_lock:
                    proxy_cache["total_checked"] = proxy_cache.get("total_checked", 0) + 1
    
    # Final validation summary (KH√îNG override cache ƒë√£ t√≠ch l≈©y)
    final_alive_count = proxy_cache.get("alive_count", 0)
    final_total_checked = proxy_cache.get("total_checked", 0)
    
    # Ch·ªâ update timestamp
    with cache_lock:
        proxy_cache["last_update"] = datetime.now().isoformat()
    
    # SIMPLIFIED: Logs ng·∫Øn g·ªçn h∆°n
    current_cycle_success = round(len(alive_proxies)/total_proxies*100, 1) if total_proxies > 0 else 0
    overall_success = round(final_alive_count/final_total_checked*100, 1) if final_total_checked > 0 else 0
    log_to_render(f"üéØ VALIDATION HO√ÄN TH√ÄNH!")
    log_to_render(f"üìä Batch n√†y: {len(alive_proxies)} alive / {total_proxies} total ({current_cycle_success}%)")
    log_to_render(f"üìà T·ªïng c·ªông: {final_alive_count} alive / {final_total_checked} total ({overall_success}%)")
    
    return alive_proxies

def check_initial_fetch_timeout(start_time, max_hours=2):
    """Ki·ªÉm tra timeout cho initial fetch ƒë·ªÉ tr√°nh ch·∫°y v√¥ t·∫≠n"""
    elapsed_hours = (time.time() - start_time) / 3600
    if elapsed_hours > max_hours:
        log_to_render(f"‚ö†Ô∏è INITIAL FETCH TIMEOUT: {elapsed_hours:.1f}h > {max_hours}h")
        log_to_render("üîÑ Force chuy·ªÉn sang MAINTENANCE MODE v·ªõi proxy hi·ªán c√≥")
        return True
    return False

def validate_existing_proxies_only():
    """Maintenance mode - ch·ªâ re-check c√°c proxy ƒë√£ c√≥ trong cache"""
    global proxy_cache
    
    # L·∫•y c√°c proxy hi·ªán c√≥ t·ª´ cache
    existing_proxies = proxy_cache.get('http', [])
    
    if not existing_proxies:
        log_to_render("‚ö†Ô∏è MAINTENANCE MODE: Kh√¥ng c√≥ proxy trong cache ƒë·ªÉ re-check")
        return []
    
    log_to_render(f"üîÑ MAINTENANCE MODE: Re-checking {len(existing_proxies)} proxy c√≥ s·∫µn...")
    
    # Chuy·ªÉn ƒë·ªïi format ƒë·ªÉ validate
    proxy_list = []
    for p in existing_proxies:
        proxy_string = f"{p['host']}:{p['port']}"
        proxy_type = p.get('type', 'http')
        protocols_info = [proxy_type]
        proxy_list.append(('maintenance', proxy_string, protocols_info))
    
    log_to_render(f"‚ö° B·∫Øt ƒë·∫ßu re-validation {len(proxy_list)} proxy...")
    
    # Validate v·ªõi max_workers cao h∆°n cho maintenance (v√¨ √≠t proxy h∆°n)
    alive_proxies = validate_proxy_batch_smart(proxy_list, max_workers=25)
    
    log_to_render(f"‚úÖ MAINTENANCE HO√ÄN TH√ÄNH: {len(alive_proxies)}/{len(proxy_list)} proxy c√≤n s·ªëng")
    
    return alive_proxies

def background_proxy_refresh():
    """Background thread v·ªõi 2 mode: Initial fetch vs Maintenance - FIXED LOGIC"""
    global proxy_cache, startup_status
    
    log_to_render("üîÑ BACKGROUND THREAD KH·ªûI ƒê·ªòNG")
    
    # Wait a bit for service to stabilize
    log_to_render("‚è≥ Waiting 10 seconds for service stabilization...")
    time.sleep(10)
    
    initial_fetch_done = False
    cycle_count = 0
    initial_start_time = None
    
    while True:
        try:
            cycle_count += 1
            log_to_render("=" * 60)
            
            if not initial_fetch_done:
                # Track initial fetch start time
                if initial_start_time is None:
                    initial_start_time = time.time()
                
                # Check timeout protection (2 gi·ªù max)
                if check_initial_fetch_timeout(initial_start_time, max_hours=2):
                    log_to_render("üö® FORCE SWITCH: Initial fetch qu√° l√¢u, chuy·ªÉn sang maintenance")
                    initial_fetch_done = True
                    sleep_time = 300
                    continue
                
                # MODE 1: INITIAL FETCH - chia chunk ƒë·ªÉ ƒë·∫£m b·∫£o ho√†n th√†nh
                log_to_render(f"üöÄ CYCLE {cycle_count}: INITIAL FETCH MODE (CHIA CHUNK)")
                elapsed_time = round((time.time() - initial_start_time) / 60, 1)
                log_to_render(f"‚è∞ Elapsed: {elapsed_time} ph√∫t (timeout: 120 ph√∫t)")
                
                # DEBUG: Log ƒë·ªÉ track cycle behavior
                if cycle_count > 1:
                    log_to_render("‚ö†Ô∏è WARNING: This is a REPEATED CYCLE - Previous cycle did not complete!")
                    log_to_render("üîç DEBUG: This should only happen if previous cycle was interrupted")
                
                log_to_render("=" * 60)
                
                start_time = time.time()
                
                # Fetch proxies t·ª´ sources (KH√îNG GI·ªöI H·∫†N)
                log_to_render("üì• Fetching TO√ÄN B·ªò proxy t·ª´ t·∫•t c·∫£ ngu·ªìn...")
                
                try:
                    proxy_list, sources_count = fetch_proxies_from_sources()
                except Exception as e:
                    log_to_render(f"‚ùå CRITICAL ERROR trong fetch_proxies_from_sources: {str(e)}")
                    log_to_render(f"üìç Traceback: {traceback.format_exc()}")
                    log_to_render("üîÑ Retry trong 5 ph√∫t...")
                    time.sleep(300)
                    continue
                
                if proxy_list:
                    total_proxies = len(proxy_list)
                    log_to_render(f"üìä Fetch th√†nh c√¥ng: {total_proxies} proxy t·ª´ {sources_count} ngu·ªìn")
                    
                    # Chia th√†nh chunks 500 proxy m·ªói l·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o complete
                    chunk_size = 500
                    chunks = [proxy_list[i:i + chunk_size] for i in range(0, len(proxy_list), chunk_size)]
                    total_chunks = len(chunks)
                    
                    log_to_render(f"üîÄ Chia th√†nh {total_chunks} chunks ({chunk_size} proxy/chunk)")
                    log_to_render("‚ö° B·∫Øt ƒë·∫ßu validation t·ª´ng chunk...")
                    
                    all_alive_proxies = []
                    completed_chunks = 0
                    
                    # FIXED: Th√™m error handling v√† recovery cho t·ª´ng chunk
                    for chunk_idx, chunk in enumerate(chunks, 1):
                        chunk_start_time = time.time()
                        log_to_render(f"üîÑ Processing chunk {chunk_idx}/{total_chunks} ({len(chunk)} proxy)...")
                        
                        # FIXED: Th√™m try-catch cho validate_proxy_batch_smart
                        try:
                            chunk_alive = validate_proxy_batch_smart(chunk, max_workers=20)
                            all_alive_proxies.extend(chunk_alive)
                            completed_chunks += 1
                            
                            chunk_time = round(time.time() - chunk_start_time, 1)
                            progress = round(completed_chunks / total_chunks * 100, 1)
                            
                            log_to_render(f"‚úÖ Chunk {chunk_idx} DONE: {len(chunk_alive)} alive in {chunk_time}s")
                            log_to_render(f"üìà Progress: {progress}% ({completed_chunks}/{total_chunks} chunks)")
                            log_to_render(f"üìä Total alive so far: {len(all_alive_proxies)}")
                            
                        except Exception as e:
                            log_to_render(f"‚ùå CRITICAL ERROR trong validate_proxy_batch_smart chunk {chunk_idx}: {str(e)}")
                            log_to_render(f"üîÑ SKIP chunk {chunk_idx} v√† ti·∫øp t·ª•c...")
                            # V·∫´n count completed ƒë·ªÉ kh√¥ng b·ªã stuck
                            completed_chunks += 1
                            
                            # REMOVED: B·ªè log debug d√†i
                        
                        # Log ƒë·∫∑c bi·ªát ƒë·ªÉ d·ªÖ track - REDUCED frequency
                        if chunk_idx % 20 == 0:  # Gi·∫£m t·ª´ 10 ‚Üí 20 chunks
                            log_to_render("=" * 40)
                            log_to_render(f"üéØ MILESTONE: Completed {completed_chunks} chunks out of {total_chunks}")
                            log_to_render(f"‚è∞ Total runtime: {round((time.time() - start_time)/60, 1)} minutes")
                            success_rate = round(len(all_alive_proxies)/(completed_chunks*chunk_size)*100, 1) if completed_chunks > 0 else 0
                            log_to_render(f"üèÜ Success rate: {success_rate}%")
                            log_to_render("=" * 40)
                        
                        # Sleep ng·∫Øn gi·ªØa c√°c chunk ƒë·ªÉ kh√¥ng overload
                        if chunk_idx < total_chunks:
                            # REMOVED: B·ªè log sleep
                            time.sleep(10)
                    
                    # Update final cache v·ªõi t·∫•t c·∫£ proxy alive
                    with cache_lock:
                        proxy_cache["http"] = all_alive_proxies.copy()
                        proxy_cache["alive_count"] = len(all_alive_proxies)
                        proxy_cache["total_checked"] = total_proxies
                        proxy_cache["sources_processed"] = sources_count
                        proxy_cache["last_update"] = datetime.now().isoformat()
                    
                    alive_proxies = all_alive_proxies
                    
                    # FIXED: Mark complete khi ƒê√É XONG H·∫æT t·∫•t c·∫£ chunks (B·∫§T K·ªÇ c√≥ proxy s·ªëng hay kh√¥ng)
                    if completed_chunks == total_chunks:
                        initial_fetch_done = True
                        log_to_render("üéâ INITIAL FETCH 100% HO√ÄN TH√ÄNH! Chuy·ªÉn sang MAINTENANCE MODE...")
                        log_to_render(f"üìä K·∫øt qu·∫£ cu·ªëi c√πng: {len(alive_proxies)} proxy s·ªëng t·ª´ {proxy_cache.get('total_checked', 0)} ƒë√£ ki·ªÉm tra")
                        log_to_render(f"üîß FIXED: Completed {completed_chunks}/{total_chunks} chunks successfully")
                    else:
                        log_to_render(f"‚ö†Ô∏è BUG DETECTED: completed_chunks={completed_chunks} != total_chunks={total_chunks}")
                        log_to_render("üîÑ L√Ω do: Logic error - n√†y kh√¥ng n√™n x·∫£y ra sau fix")
                        log_to_render("üö® FORCE COMPLETE ƒë·ªÉ tr√°nh infinite loop")
                        initial_fetch_done = True  # FORCE complete ƒë·ªÉ tr√°nh loop
                        sleep_time = 300
                    
                else:
                    log_to_render("‚ùå INITIAL FETCH TH·∫§T B·∫†I: Kh√¥ng fetch ƒë∆∞·ª£c proxy n√†o")
                    log_to_render("üîÑ Th·ª≠ l·∫°i trong 5 ph√∫t...")
                    alive_proxies = []
                    sleep_time = 300  # 5 ph√∫t retry
                    
            else:
                # MODE 2: MAINTENANCE - ch·ªâ re-check proxy c√≥ s·∫µn
                log_to_render(f"üîß CYCLE {cycle_count}: MAINTENANCE MODE (RE-CHECK)")
                log_to_render("=" * 60)
                
                start_time = time.time()
                
                # Ch·ªâ re-check proxy c√≥ s·∫µn
                try:
                    alive_proxies = validate_existing_proxies_only()
                except Exception as e:
                    log_to_render(f"‚ùå ERROR trong validate_existing_proxies_only: {str(e)}")
                    log_to_render(f"üìç Traceback: {traceback.format_exc()}")
                    alive_proxies = []
                
                # Kh√¥ng c·∫ßn update sources_processed trong maintenance mode
            
            # T√≠nh to√°n th·ªëng k√™ cho c·∫£ 2 mode
            cycle_time = round(time.time() - start_time, 1)
            
            if not initial_fetch_done:
                # Stats cho initial mode
                total_checked = proxy_cache.get("total_checked", 0)
                success_rate = round(len(alive_proxies)/total_checked*100, 1) if total_checked > 0 else 0
                
                if initial_fetch_done:  # Ch·ªâ update startup khi th·ª±c s·ª± done
                    with cache_lock:
                        startup_status["first_fetch_completed"] = True
                
                log_to_render("=" * 60)
                if initial_fetch_done:
                    log_to_render("üéâ INITIAL FETCH 100% HO√ÄN TH√ÄNH!")
                    log_to_render(f"‚è±Ô∏è Th·ªùi gian: {cycle_time}s")
                    log_to_render(f"üìä K·∫øt qu·∫£: {len(alive_proxies)} alive / {total_checked} total")
                    log_to_render(f"üìà T·ª∑ l·ªá th√†nh c√¥ng: {success_rate}%")
                    log_to_render("üîÑ Chuy·ªÉn sang MAINTENANCE MODE...")
                    sleep_time = 300  # 5 ph√∫t cho maintenance mode ƒë·∫ßu ti√™n
                else:
                    log_to_render("‚ö†Ô∏è INITIAL FETCH CH∆ØA HO√ÄN TH√ÄNH")
                    log_to_render(f"‚è±Ô∏è Th·ªùi gian cycle: {cycle_time}s")
                    log_to_render(f"üìä Progress: {len(alive_proxies)} alive / {total_checked} checked")
                    log_to_render("üîÑ Ti·∫øp t·ª•c INITIAL MODE...")
                    sleep_time = 300  # 5 ph√∫t retry
                log_to_render("=" * 60)
                
            else:
                # Stats cho maintenance mode  
                existing_count = len(proxy_cache.get('http', []))
                
                if existing_count == 0:
                    log_to_render("‚ö†Ô∏è MAINTENANCE: Kh√¥ng c√≥ proxy ƒë·ªÉ check, quay l·∫°i INITIAL MODE")
                    initial_fetch_done = False
                    sleep_time = 60  # 1 ph√∫t
                    continue
                    
                success_rate = round(len(alive_proxies)/existing_count*100, 1) if existing_count > 0 else 0
                
                log_to_render("=" * 60)
                log_to_render("‚úÖ MAINTENANCE HO√ÄN TH√ÄNH!")
                log_to_render(f"‚è±Ô∏è Th·ªùi gian: {cycle_time}s")
                log_to_render(f"üìä K·∫øt qu·∫£: {len(alive_proxies)} alive / {existing_count} total")
                log_to_render(f"üìà T·ª∑ l·ªá c√≤n s·ªëng: {success_rate}%")
                log_to_render("üîÑ Ti·∫øp theo trong 10 ph√∫t...")
                log_to_render("=" * 60)
                
                # Sleep b√¨nh th∆∞·ªùng cho maintenance
                sleep_time = 600  # 10 ph√∫t
            
        except Exception as e:
            log_to_render(f"‚ùå L·ªñI BACKGROUND REFRESH: {str(e)}")
            log_to_render(f"üìç Traceback: {traceback.format_exc()}")
            log_to_render("üîÑ Ti·∫øp t·ª•c v√≤ng l·∫∑p...")
            startup_status["error_count"] += 1
            sleep_time = 300  # 5 ph√∫t n·∫øu c√≥ l·ªói
            
            # FIXED: Th√™m protection ƒë·ªÉ tr√°nh infinite loop
            if startup_status["error_count"] > 10:
                log_to_render("üö® CRITICAL: Qu√° nhi·ªÅu l·ªói li√™n ti·∫øp, FORCE SWITCH sang maintenance")
                initial_fetch_done = True
                startup_status["error_count"] = 0
        
        # Sleep v·ªõi th·ªùi gian ƒë·ªông theo mode
        sleep_minutes = sleep_time // 60
        log_to_render(f"üò¥ Sleep {sleep_minutes} ph√∫t tr∆∞·ªõc chu k·ª≥ ti·∫øp theo...")
        time.sleep(sleep_time)

# Initialize service when Flask starts
initialize_service()

@app.route('/')
def home():
    """UI ch√≠nh v·ªõi real-time logs"""
    html = f"""
    <!DOCTYPE html>
    <html lang="vi">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Proxy Validation Service - Real Time</title>
        <style>
            body {{
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                margin: 0;
                padding: 20px;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
                color: white;
            }}
            .container {{
                max-width: 1400px;
                margin: 0 auto;
                background: rgba(255, 255, 255, 0.1);
                backdrop-filter: blur(10px);
                border-radius: 20px;
                padding: 30px;
                box-shadow: 0 8px 32px rgba(31, 38, 135, 0.37);
                border: 1px solid rgba(255, 255, 255, 0.18);
            }}
            .header {{
                text-align: center;
                margin-bottom: 30px;
                padding-bottom: 20px;
                border-bottom: 2px solid rgba(255, 255, 255, 0.2);
            }}
            .header h1 {{
                margin: 0;
                font-size: 2.5em;
                background: linear-gradient(45deg, #fff, #f0f0f0);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            }}
            .status {{
                text-align: center;
                padding: 15px;
                border-radius: 10px;
                margin: 20px 0;
                font-weight: bold;
                font-size: 1.2em;
            }}
            .status-success {{
                background: rgba(76, 175, 80, 0.3);
                border: 2px solid #4CAF50;
            }}
            .status-info {{
                background: rgba(33, 150, 243, 0.3);
                border: 2px solid #2196F3;
            }}
            .status-error {{
                background: rgba(244, 67, 54, 0.3);
                border: 2px solid #f44336;
            }}
            .grid {{
                display: grid;
                grid-template-columns: 1fr 1fr;
                gap: 20px;
                margin-top: 30px;
            }}
            .card {{
                background: rgba(255, 255, 255, 0.1);
                border-radius: 15px;
                padding: 20px;
                border: 1px solid rgba(255, 255, 255, 0.2);
            }}
            .card h3 {{
                margin-top: 0;
                color: #fff;
                border-bottom: 2px solid rgba(255, 255, 255, 0.3);
                padding-bottom: 10px;
            }}
            .logs-container {{
                grid-column: 1 / -1;
                background: rgba(0, 0, 0, 0.3);
                border-radius: 15px;
                padding: 20px;
                max-height: 500px;
                overflow-y: auto;
            }}
            .log-entry {{
                font-family: 'Courier New', monospace;
                font-size: 0.9em;
                margin: 2px 0;
                padding: 3px 8px;
                border-radius: 4px;
                word-wrap: break-word;
            }}
            .log-INFO {{ background: rgba(33, 150, 243, 0.2); }}
            .log-ERROR {{ background: rgba(244, 67, 54, 0.3); }}
            .log-SUCCESS {{ background: rgba(76, 175, 80, 0.2); }}
            .update-time {{
                font-size: 0.9em;
                opacity: 0.8;
                text-align: center;
                margin-top: 20px;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>üöÄ Proxy Validation Service</h1>
                <p>Real-time monitoring v√† logging - T·ªëi ∆∞u cho Render Free Plan</p>
            </div>
            
            <div id="system-status" class="status status-error">
                <div id="current-status">ƒêang kh·ªüi ƒë·ªông service...</div>
            </div>
            
            <div class="grid">
                <div class="card">
                    <h3>üìä Th·ªëng K√™ Proxy</h3>
                    <div id="stats">
                        <p>ƒêang t·∫£i...</p>
                    </div>
                </div>
                
                <div class="card">
                    <h3>‚öôÔ∏è Tr·∫°ng Th√°i H·ªá Th·ªëng</h3>
                    <div id="system-info">
                        <p>ƒêang t·∫£i...</p>
                    </div>
                </div>
                
                <div class="logs-container">
                    <h3>üìú Real-Time Logs</h3>
                    <div id="logs" style="max-height: 400px; overflow-y: auto;">
                        <p>ƒêang t·∫£i logs...</p>
                    </div>
                </div>
            </div>
            
            <div class="update-time">
                <p>‚è±Ô∏è T·ª± ƒë·ªông c·∫≠p nh·∫≠t m·ªói 5 gi√¢y | üîÑ Logs real-time</p>
                <p>üìä Service monitoring v·ªõi chi ti·∫øt t·ª´ng b∆∞·ªõc</p>
            </div>
        </div>
        
        <script>
            function updateStats() {{
                fetch('/api/proxy/stats')
                    .then(response => response.json())
                    .then(data => {{
                        // Update stats
                        document.getElementById('stats').innerHTML = 
                            '<p><strong>Proxy s·ªëng:</strong> ' + data.alive_count + '</p>' +
                            '<p><strong>T·ªïng ƒë√£ check:</strong> ' + data.total_checked + '</p>' +
                            '<p><strong>T·ª∑ l·ªá th√†nh c√¥ng:</strong> ' + data.success_rate + '%</p>' +
                            '<p><strong>Ngu·ªìn ƒë√£ x·ª≠ l√Ω:</strong> ' + data.sources_processed + '/' + data.sources_count + '</p>' +
                            '<p><strong>L·∫ßn check cu·ªëi:</strong> ' + (data.last_update ? new Date(data.last_update).toLocaleString() : 'Ch∆∞a check') + '</p>';
                        
                        // Update status
                        const statusEl = document.getElementById('current-status');
                        const statusContainer = document.getElementById('system-status');
                        
                        if (data.alive_count > 50) {{
                            statusEl.textContent = 'Service ho·∫°t ƒë·ªông t·ªët - ' + data.alive_count + ' proxy s·ªëng';
                            statusContainer.className = 'status status-success';
                        }} else if (data.alive_count > 0) {{
                            statusEl.textContent = 'Service ho·∫°t ƒë·ªông - ' + data.alive_count + ' proxy s·ªëng';
                            statusContainer.className = 'status status-info';
                        }} else {{
                            statusEl.textContent = 'ƒêang t√¨m proxy s·ªëng...';
                            statusContainer.className = 'status status-error';
                        }}
                    }})
                    .catch(e => {{
                        document.getElementById('stats').innerHTML = '<p style="color: red;">Service ƒëang kh·ªüi ƒë·ªông...</p>';
                    }});
            }}
            
            function updateLogs() {{
                fetch('/api/logs')
                    .then(response => response.json())
                    .then(data => {{
                        const logsContainer = document.getElementById('logs');
                        
                        if (data.logs && data.logs.length > 0) {{
                            logsContainer.innerHTML = data.logs.map(log => 
                                '<div class="log-entry log-' + log.level + '">' + log.full_log + '</div>'
                            ).join('');
                            logsContainer.scrollTop = logsContainer.scrollHeight;
                        }}
                        
                        // Update system info
                        document.getElementById('system-info').innerHTML = 
                            '<p><strong>Kh·ªüi t·∫°o:</strong> ' + (data.startup.initialized ? '‚úÖ' : '‚ùå') + '</p>' +
                            '<p><strong>Background Thread:</strong> ' + (data.startup.background_thread_started ? '‚úÖ' : '‚ùå') + '</p>' +
                            '<p><strong>First Fetch:</strong> ' + (data.startup.first_fetch_completed ? '‚úÖ' : '‚ùå') + '</p>' +
                            '<p><strong>Errors:</strong> ' + data.startup.error_count + '</p>' +
                            '<p><strong>Ho·∫°t ƒë·ªông cu·ªëi:</strong> ' + (data.startup.last_activity ? new Date(data.startup.last_activity).toLocaleTimeString() : 'N/A') + '</p>';
                    }})
                    .catch(e => {{
                        console.log('Error fetching logs:', e);
                    }});
            }}
            
            // Update every 5 seconds
            updateStats();
            updateLogs();
            setInterval(updateStats, 5000);
            setInterval(updateLogs, 3000);  // Logs update faster
        </script>
    </body>
    </html>
    """
    return html

@app.route('/api/logs', methods=['GET'])
def get_logs():
    """API ƒë·ªÉ l·∫•y real-time logs"""
    try:
        # REMOVED: B·ªè log kh√¥ng c·∫ßn thi·∫øt
        
        # Return recent logs
        recent_logs = list(log_buffer)[-100:]  # Last 100 logs
        
        return jsonify({
            'success': True,
            'logs': recent_logs,
            'total_logs': len(log_buffer),
            'startup': startup_status,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/proxy/alive', methods=['GET'])
def get_alive_proxies():
    """API ch√≠nh - l·∫•y danh s√°ch proxy s·ªëng"""
    try:
        count = int(request.args.get('count', 50))
        
        alive_proxies = proxy_cache.get('http', [])
        
        # Sort by speed (fastest first)
        sorted_proxies = sorted(alive_proxies, key=lambda x: x.get('speed', 999))
        
        # Return requested count
        result_proxies = sorted_proxies[:count]
        
        return jsonify({
            'success': True,
            'total_available': len(alive_proxies),
            'returned_count': len(result_proxies),
            'proxies': result_proxies,
            'last_update': proxy_cache.get('last_update'),
            'timestamp': datetime.now().isoformat(),
            'sources_count': len(PROXY_SOURCE_LINKS["categorized"]) + len(PROXY_SOURCE_LINKS["mixed"]),
            'cache_alive_count': proxy_cache.get('alive_count', 0)
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/proxy/stats', methods=['GET'])
def get_proxy_stats():
    """API th·ªëng k√™ proxy v·ªõi th√¥ng tin chi ti·∫øt"""
    try:
        # REMOVED: B·ªè debug logs kh√¥ng c·∫ßn thi·∫øt
        
        last_update = proxy_cache.get('last_update')
        cache_age_minutes = 0
        
        if last_update:
            last_update_dt = datetime.fromisoformat(last_update)
            cache_age_minutes = int((datetime.now() - last_update_dt).total_seconds() / 60)
        
        # Get from cache - now properly updated
        total_checked = proxy_cache.get('total_checked', 0)
        alive_count = proxy_cache.get('alive_count', 0)
        success_rate = round(alive_count / total_checked * 100, 2) if total_checked > 0 else 0
        
        # Count total sources
        total_sources = len(PROXY_SOURCE_LINKS["categorized"]) + len(PROXY_SOURCE_LINKS["mixed"])
        
        # REMOVED: B·ªè debug logs chi ti·∫øt
        
        return jsonify({
            'success': True,
            'alive_count': alive_count,
            'total_checked': total_checked,
            'success_rate': success_rate,
            'last_update': last_update,
            'cache_age_minutes': cache_age_minutes,
            'sources_count': total_sources,
            'sources_processed': proxy_cache.get('sources_processed', 0),
            'categorized_sources': list(PROXY_SOURCE_LINKS["categorized"].keys()),
            'mixed_sources': list(PROXY_SOURCE_LINKS["mixed"].keys()),
            'service_status': 'render_free_optimized',
            'check_interval': '10 minutes',
            'timeout_setting': '6 seconds',
            'max_workers': 15,
            'processing_mode': 'CHUNK_PROCESSING_800_MAX',
            'chunk_size': 300,
            'render_plan': 'free_512mb'
        })
        
    except Exception as e:
        log_to_render(f"‚ùå L·ªói API stats: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/proxies', methods=['GET'])
def get_proxies_simple():
    """API ƒë∆°n gi·∫£n ƒë·ªÉ l·∫•y proxy s·ªëng - cho tool s·ª≠ d·ª•ng"""
    try:
        count = int(request.args.get('count', 100))
        format_type = request.args.get('format', 'json')  # json ho·∫∑c text
        
        # L·∫•y t·ª´ cache ho·∫∑c tr·∫£ v·ªÅ empty n·∫øu cache r·ªóng
        alive_proxies = proxy_cache.get('http', [])
        
        if not alive_proxies:
            # REMOVED: B·ªè debug log kh√¥ng c·∫ßn thi·∫øt
            return jsonify({
                'success': False,
                'message': 'No live proxies available yet. Service is still validating.',
                'count': 0,
                'proxies': [],
                'cache_status': 'empty'
            })
        
        # Sort by speed v√† l·∫•y s·ªë l∆∞·ª£ng y√™u c·∫ßu
        sorted_proxies = sorted(alive_proxies, key=lambda x: x.get('speed', 999))[:count]
        
        if format_type == 'text':
            # Format text: host:port per line
            proxy_list = [f"{p['host']}:{p['port']}" for p in sorted_proxies]
            return '\n'.join(proxy_list), 200, {'Content-Type': 'text/plain'}
        
        # Format JSON (default)
        return jsonify({
            'success': True,
            'count': len(sorted_proxies),
            'total_available': len(alive_proxies),
            'proxies': [
                {
                    'host': p['host'],
                    'port': p['port'],
                    'type': p['type'],
                    'speed': p['speed'],
                    'proxy': f"{p['host']}:{p['port']}"
                } for p in sorted_proxies
            ],
            'last_update': proxy_cache.get('last_update')
        })
        
    except Exception as e:
        log_to_render(f"‚ùå API /proxies error: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/debug', methods=['GET'])
def debug_cache():
    """Debug cache ƒë·ªÉ ki·ªÉm tra v·∫•n ƒë·ªÅ"""
    # REMOVED: B·ªè log debug kh√¥ng c·∫ßn thi·∫øt
    
    return jsonify({
        'process_id': os.getpid(),
        'proxy_cache': proxy_cache,
        'cache_http_length': len(proxy_cache.get('http', [])),
        'cache_alive_count': proxy_cache.get('alive_count', 0),
        'cache_total_checked': proxy_cache.get('total_checked', 0),
        'startup_status': startup_status,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint ƒë·ªÉ test service"""
    # REMOVED: B·ªè log kh√¥ng c·∫ßn thi·∫øt
    
    # T√≠nh to√°n m·ªôt s·ªë metrics h·ªØu √≠ch
    alive_count = len(proxy_cache.get('http', []))
    total_checked = proxy_cache.get('total_checked', 0)
    success_rate = round(alive_count/total_checked*100, 1) if total_checked > 0 else 0
    
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'cache_count': alive_count,
        'service': 'proxy-validation-render',
        'startup_status': startup_status,
        'metrics': {
            'alive_proxies': alive_count,
            'total_checked': total_checked,
            'success_rate_percent': success_rate,
            'last_update': proxy_cache.get('last_update', 'Never')
        },
        'fixed_bugs': [
            'Loop bug fixed - no more restart from chunk 1',
            'Condition fixed - completes after all chunks processed'
        ]
    })

@app.route('/api/force/initial', methods=['POST'])
def force_initial_mode():
    """Force switch v·ªÅ initial fetch mode"""
    try:
        # Note: Trong production, c·∫ßn implement proper global state management
        log_to_render("üîÑ API TRIGGER: Force switch v·ªÅ INITIAL FETCH MODE requested")
        
        return jsonify({
            'success': True,
            'message': 'Signal sent to switch to initial fetch mode on next cycle',
            'note': 'Change will take effect in next background cycle',
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    # Render production mode
    try:
        port = int(os.environ.get('PORT', 5000))
        log_to_render("üöÄ STARTING RENDER PRODUCTION MODE")
        log_to_render(f"üîß Port: {port}")
        app.run(host='0.0.0.0', port=port, debug=False, threaded=True)
    except Exception as e:
        log_to_render(f"‚ùå L·ªñI PRODUCTION: {str(e)}")
        print(f"Error: {e}") 